{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nclass SelectiveScan(nn.Module):\\n    def __init__(self, d_model, d_state=16, d_conv=4):\\n        super().__init__()\\n        self.d_model = d_model\\n        self.d_state = d_state\\n        self.d_conv = d_conv\\n        \\n        # Parameters for state space model\\n        self.A = nn.Parameter(torch.randn(d_model, d_state, d_state) / d_state)\\n        self.B = nn.Parameter(torch.randn(d_model, d_state) / math.sqrt(d_state))\\n        self.C = nn.Parameter(torch.randn(d_model, d_state) / math.sqrt(d_state))\\n        \\n        # Convolution for local context\\n        self.conv = nn.Conv1d(\\n            in_channels=d_model,\\n            out_channels=d_model,\\n            kernel_size=d_conv,\\n            padding=d_conv-1,\\n            groups=d_model\\n        )\\n        \\n        # Delta and gamma parameters for selective scanning\\n        self.dt = nn.Parameter(torch.randn(d_model))\\n        self.gamma = nn.Parameter(torch.randn(d_model))\\n\\n    def forward(self, x):\\n        B, L, D = x.shape\\n        \\n        # Local context processing\\n        x_conv = rearrange(x, \\'b l d -> b d l\\')\\n        x_conv = self.conv(x_conv)[..., :L]\\n        x_conv = rearrange(x_conv, \\'b d l -> b l d\\')\\n        \\n        # Initialize state\\n        h = torch.zeros(B, self.d_state, device=x.device)\\n        outputs = []\\n        \\n        # Selective scan\\n        for t in range(L):\\n            # Current input\\n            u = x[:, t]\\n            \\n            # Update state with selective scan\\n            delta = torch.sigmoid(self.dt)\\n            A_hat = torch.exp(self.A * delta.unsqueeze(-1).unsqueeze(-1))\\n            h = torch.einsum(\\'bm,mdh->bh\\', u, A_hat) + h\\n            \\n            # Compute output\\n            y = torch.einsum(\\'bh,mh->bm\\', h, self.C)\\n            y = y * torch.sigmoid(self.gamma).unsqueeze(0)\\n            \\n            outputs.append(y)\\n        \\n        outputs = torch.stack(outputs, dim=1)\\n        return outputs + x_conv\\n\\nclass MambaBlock(nn.Module):\\n    def __init__(self, d_model, d_state=16, d_conv=4):\\n        super().__init__()\\n        self.norm = nn.LayerNorm(d_model)\\n        self.proj_in = nn.Linear(d_model, d_model * 2)\\n        self.selective_scan = SelectiveScan(d_model, d_state, d_conv)\\n        self.proj_out = nn.Linear(d_model, d_model)\\n        \\n    def forward(self, x):\\n        residual = x\\n        x = self.norm(x)\\n        \\n        # Project and split into value and gating branches\\n        x = self.proj_in(x)\\n        v, g = x.chunk(2, dim=-1)\\n        \\n        # Apply selective scan to value branch\\n        v = self.selective_scan(v)\\n        \\n        # Apply gating\\n        x = v * torch.sigmoid(g)\\n        \\n        # Project out and add residual\\n        x = self.proj_out(x)\\n        return x + residual\\n\\nclass AttentionBlock(nn.Module):\\n    def __init__(self, channels):\\n        super().__init__()\\n        self.mha = nn.MultiheadAttention(channels, 8, batch_first=True)\\n        self.norm = nn.LayerNorm(channels)\\n        \\n    def forward(self, x):\\n        b, c, h, w = x.shape\\n        x = rearrange(x, \\'b c h w -> b (h w) c\\')\\n        x = self.norm(x)\\n        attn_out, _ = self.mha(x, x, x)\\n        x = x + attn_out\\n        x = rearrange(x, \\'b (h w) c -> b c h w\\', h=h, w=w)\\n        return x\\n\\nclass DownBlock(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super().__init__()\\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\\n        self.mamba = MambaBlock(out_channels)\\n        self.attention = AttentionBlock(out_channels)\\n        self.fusion = nn.Conv2d(out_channels * 2, out_channels, 1)\\n        self.norm = nn.BatchNorm2d(out_channels)\\n        self.act = nn.GELU()\\n        \\n    def forward(self, x):\\n        x = self.conv(x)\\n        \\n        # Mamba path\\n        b, c, h, w = x.shape\\n        x_mamba = rearrange(x, \\'b c h w -> b (h w) c\\')\\n        x_mamba = self.mamba(x_mamba)\\n        x_mamba = rearrange(x_mamba, \\'b (h w) c -> b c h w\\', h=h, w=w)\\n        \\n        # Attention path\\n        x_attn = self.attention(x)\\n        \\n        # Fusion\\n        combined = torch.cat([x_mamba, x_attn], dim=1)\\n        out = self.fusion(combined)\\n        out = self.norm(out)\\n        out = self.act(out)\\n        return out\\n\\nclass UpBlock(nn.Module):\\n    def __init__(self, in_channels, out_channels):\\n        super().__init__()\\n        self.up = nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2)\\n        self.conv = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\\n        self.mamba = MambaBlock(out_channels)\\n        self.attention = AttentionBlock(out_channels)\\n        self.fusion = nn.Conv2d(out_channels * 2, out_channels, 1)\\n        self.norm = nn.BatchNorm2d(out_channels)\\n        self.act = nn.GELU()\\n        \\n    def forward(self, x, skip):\\n        x = self.up(x)\\n        x = torch.cat([x, skip], dim=1)\\n        x = self.conv(x)\\n        \\n        # Mamba path\\n        b, c, h, w = x.shape\\n        x_mamba = rearrange(x, \\'b c h w -> b (h w) c\\')\\n        x_mamba = self.mamba(x_mamba)\\n        x_mamba = rearrange(x_mamba, \\'b (h w) c -> b c h w\\', h=h, w=w)\\n        \\n        # Attention path\\n        x_attn = self.attention(x)\\n        \\n        # Fusion\\n        combined = torch.cat([x_mamba, x_attn], dim=1)\\n        out = self.fusion(combined)\\n        out = self.norm(out)\\n        out = self.act(out)\\n        return out\\n\\nclass MambaAttentionUNET(nn.Module):\\n    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\\n        super().__init__()\\n        self.downs = nn.ModuleList()\\n        self.ups = nn.ModuleList()\\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\\n\\n        # Encoder path\\n        in_features = in_channels\\n        for feature in features:\\n            self.downs.append(DownBlock(in_features, feature))\\n            in_features = feature\\n\\n        # Bottleneck\\n        self.bottleneck = DownBlock(features[-1], features[-1] * 2)\\n\\n        # Decoder path\\n        for feature in reversed(features):\\n            self.ups.append(UpBlock(feature * 2, feature))\\n\\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\\n\\n    def forward(self, x):\\n        skip_connections = []\\n\\n        # Encoder\\n        down_out = x\\n        for down in self.downs:\\n            down_out = down(down_out)\\n            skip_connections.append(down_out)\\n            down_out = self.pool(down_out)\\n\\n        # Bottleneck\\n        x = self.bottleneck(down_out)\\n\\n        # Decoder\\n        skip_connections = skip_connections[::-1]\\n        for idx in range(len(self.ups)):\\n            skip = skip_connections[idx]\\n            x = self.ups[idx](x, skip)\\n\\n        return self.final_conv(x)\\n\\nclass Trainer:\\n    def __init__(self, model, device=\\'cuda\\'):\\n        self.model = model.to(device)\\n        self.device = device\\n        self.criterion = nn.MSELoss()\\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\\n        \\n    def train_epoch(self, train_loader):\\n        self.model.train()\\n        total_loss = 0\\n        \\n        for batch_idx, (data, target) in enumerate(train_loader):\\n            data, target = data.to(self.device), target.to(self.device)\\n            \\n            self.optimizer.zero_grad()\\n            output = self.model(data)\\n            loss = self.criterion(output, target)\\n            \\n            loss.backward()\\n            self.optimizer.step()\\n            \\n            total_loss += loss.item()\\n            \\n            if batch_idx % 10 == 0:\\n                print(f\\'Batch: {batch_idx}, Loss: {loss.item():.6f}\\')\\n                \\n        return total_loss / len(train_loader)\\n    \\n    def validate(self, val_loader):\\n        self.model.eval()\\n        total_loss = 0\\n        \\n        with torch.no_grad():\\n            for data, target in val_loader:\\n                data, target = data.to(self.device), target.to(self.device)\\n                output = self.model(data)\\n                loss = self.criterion(output, target)\\n                total_loss += loss.item()\\n                \\n        return total_loss / len(val_loader)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "\n",
    "class SelectiveScan(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        \n",
    "        # Parameters for state space model\n",
    "        self.A = nn.Parameter(torch.randn(d_model, d_state, d_state) / d_state)\n",
    "        self.B = nn.Parameter(torch.randn(d_model, d_state) / math.sqrt(d_state))\n",
    "        self.C = nn.Parameter(torch.randn(d_model, d_state) / math.sqrt(d_state))\n",
    "        \n",
    "        # Convolution for local context\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv-1,\n",
    "            groups=d_model\n",
    "        )\n",
    "        \n",
    "        # Delta and gamma parameters for selective scanning\n",
    "        self.dt = nn.Parameter(torch.randn(d_model))\n",
    "        self.gamma = nn.Parameter(torch.randn(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Local context processing\n",
    "        x_conv = rearrange(x, 'b l d -> b d l')\n",
    "        x_conv = self.conv(x_conv)[..., :L]\n",
    "        x_conv = rearrange(x_conv, 'b d l -> b l d')\n",
    "        \n",
    "        # Initialize state\n",
    "        h = torch.zeros(B, self.d_state, device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        # Selective scan\n",
    "        for t in range(L):\n",
    "            # Current input\n",
    "            u = x[:, t]\n",
    "            \n",
    "            # Update state with selective scan\n",
    "            delta = torch.sigmoid(self.dt)\n",
    "            A_hat = torch.exp(self.A * delta.unsqueeze(-1).unsqueeze(-1))\n",
    "            h = torch.einsum('bm,mdh->bh', u, A_hat) + h\n",
    "            \n",
    "            # Compute output\n",
    "            y = torch.einsum('bh,mh->bm', h, self.C)\n",
    "            y = y * torch.sigmoid(self.gamma).unsqueeze(0)\n",
    "            \n",
    "            outputs.append(y)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs + x_conv\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.proj_in = nn.Linear(d_model, d_model * 2)\n",
    "        self.selective_scan = SelectiveScan(d_model, d_state, d_conv)\n",
    "        self.proj_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Project and split into value and gating branches\n",
    "        x = self.proj_in(x)\n",
    "        v, g = x.chunk(2, dim=-1)\n",
    "        \n",
    "        # Apply selective scan to value branch\n",
    "        v = self.selective_scan(v)\n",
    "        \n",
    "        # Apply gating\n",
    "        x = v * torch.sigmoid(g)\n",
    "        \n",
    "        # Project out and add residual\n",
    "        x = self.proj_out(x)\n",
    "        return x + residual\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(channels, 8, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        attn_out, _ = self.mha(x, x, x)\n",
    "        x = x + attn_out\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return x\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.mamba = MambaBlock(out_channels)\n",
    "        self.attention = AttentionBlock(out_channels)\n",
    "        self.fusion = nn.Conv2d(out_channels * 2, out_channels, 1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Mamba path\n",
    "        b, c, h, w = x.shape\n",
    "        x_mamba = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x_mamba = self.mamba(x_mamba)\n",
    "        x_mamba = rearrange(x_mamba, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        # Attention path\n",
    "        x_attn = self.attention(x)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([x_mamba, x_attn], dim=1)\n",
    "        out = self.fusion(combined)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2)\n",
    "        self.conv = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\n",
    "        self.mamba = MambaBlock(out_channels)\n",
    "        self.attention = AttentionBlock(out_channels)\n",
    "        self.fusion = nn.Conv2d(out_channels * 2, out_channels, 1)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Mamba path\n",
    "        b, c, h, w = x.shape\n",
    "        x_mamba = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x_mamba = self.mamba(x_mamba)\n",
    "        x_mamba = rearrange(x_mamba, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        # Attention path\n",
    "        x_attn = self.attention(x)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([x_mamba, x_attn], dim=1)\n",
    "        out = self.fusion(combined)\n",
    "        out = self.norm(out)\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "class MambaAttentionUNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Encoder path\n",
    "        in_features = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(DownBlock(in_features, feature))\n",
    "            in_features = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DownBlock(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Decoder path\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(UpBlock(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Encoder\n",
    "        down_out = x\n",
    "        for down in self.downs:\n",
    "            down_out = down(down_out)\n",
    "            skip_connections.append(down_out)\n",
    "            down_out = self.pool(down_out)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(down_out)\n",
    "\n",
    "        # Decoder\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(len(self.ups)):\n",
    "            skip = skip_connections[idx]\n",
    "            x = self.ups[idx](x, skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch: {batch_idx}, Loss: {loss.item():.6f}')\n",
    "                \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def test_model():\\n    # Create sample input\\n    batch_size = 2\\n    channels = 3\\n    height = width = 256\\n    \\n    # Initialize model\\n    model = MambaAttentionUNET(in_channels=channels, out_channels=1)\\n    x = torch.randn(batch_size, channels, height, width)\\n    \\n    # Forward pass\\n    output = model(x)\\n    \\n    print(f\"Input shape: {x.shape}\")\\n    print(f\"Output shape: {output.shape}\")\\n    \\n    return model, output\\n\\nif __name__ == \"__main__\":\\n    # Test the model\\n    print(\"Testing model...\")\\n    model, output = test_model()\\n    print(\"Model test successful!\")\\n    \\n    # Example of creating a training dataset\\n    print(\"\\nCreating example dataset...\")\\n    num_samples = 100\\n    input_data = torch.randn(num_samples, 3, 256, 256)\\n    target_data = torch.randn(num_samples, 1, 256, 256)\\n    \\n    # Create DataLoader\\n    from torch.utils.data import TensorDataset, DataLoader\\n    dataset = TensorDataset(input_data, target_data)\\n    train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\\n    \\n    # Initialize trainer\\n    print(\"\\nInitializing trainer...\")\\n    trainer = Trainer(model, device=\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n    \\n    # Train for a few epochs\\n    print(\"\\nStarting training...\")\\n    num_epochs = 3\\n    for epoch in range(num_epochs):\\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\\n        avg_loss = trainer.train_epoch(train_loader)\\n        print(f\"Average loss: {avg_loss:.6f}\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def test_model():\n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    channels = 3\n",
    "    height = width = 256\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MambaAttentionUNET(in_channels=channels, out_channels=1)\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    return model, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the model\n",
    "    print(\"Testing model...\")\n",
    "    model, output = test_model()\n",
    "    print(\"Model test successful!\")\n",
    "    \n",
    "    # Example of creating a training dataset\n",
    "    print(\"\\nCreating example dataset...\")\n",
    "    num_samples = 100\n",
    "    input_data = torch.randn(num_samples, 3, 256, 256)\n",
    "    target_data = torch.randn(num_samples, 1, 256, 256)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    dataset = TensorDataset(input_data, target_data)\n",
    "    train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = Trainer(model, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Train for a few epochs\n",
    "    print(\"\\nStarting training...\")\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        avg_loss = trainer.train_epoch(train_loader)\n",
    "        print(f\"Average loss: {avg_loss:.6f}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with small input...\n",
      "Input shape: torch.Size([2, 3, 64, 64])\n",
      "Output shape: torch.Size([2, 1, 64, 64])\n",
      "Model is on: cpu\n",
      "Small input test successful!\n",
      "\n",
      "Testing model with medium input...\n",
      "Input shape: torch.Size([2, 3, 128, 128])\n",
      "Output shape: torch.Size([2, 1, 128, 128])\n",
      "Model is on: cpu\n",
      "Medium input test successful!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EfficientAttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction_factor=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        reduced_dim = max(32, channels // reduction_factor)\n",
    "        self.qkv = nn.Linear(channels, reduced_dim * 3)\n",
    "        self.proj = nn.Linear(reduced_dim, channels)\n",
    "        self.chunk_size = 256  # Process attention in chunks\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Reduce sequence length for efficiency\n",
    "        qkv = self.qkv(x)\n",
    "        chunk_size = min(self.chunk_size, qkv.shape[1])\n",
    "        \n",
    "        outputs = []\n",
    "        for chunk_idx in range(0, x.shape[1], chunk_size):\n",
    "            chunk_end = min(chunk_idx + chunk_size, x.shape[1])\n",
    "            qkv_chunk = qkv[:, chunk_idx:chunk_end]\n",
    "            \n",
    "            q, k, v = qkv_chunk.chunk(3, dim=-1)\n",
    "            \n",
    "            # Scaled dot-product attention\n",
    "            attn = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            chunk_output = attn @ v\n",
    "            outputs.append(chunk_output)\n",
    "        \n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        output = self.proj(output)\n",
    "        output = output + x  # Residual connection\n",
    "        \n",
    "        output = rearrange(output, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return output\n",
    "\n",
    "class SimplifiedMambaBlock(nn.Module):\n",
    "    def __init__(self, channels, d_state=16):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        self.conv1 = nn.Conv1d(channels, channels, 3, padding=1, groups=channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Reshape and normalize\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b n c -> b c n')\n",
    "        \n",
    "        # Simplified SSM using convolutions\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = rearrange(x, 'b c n -> b n c')\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        \n",
    "        return x + residual\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.mamba = SimplifiedMambaBlock(out_channels)\n",
    "        self.attention = EfficientAttentionBlock(out_channels)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.mamba(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2)\n",
    "        self.conv = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\n",
    "        self.mamba = SimplifiedMambaBlock(out_channels)\n",
    "        self.attention = EfficientAttentionBlock(out_channels)\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.mamba(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class MambaAttentionUNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part\n",
    "        in_features = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(DownBlock(in_features, feature))\n",
    "            in_features = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DownBlock(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Up part\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(UpBlock(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Encoder\n",
    "        down_out = x\n",
    "        for down in self.downs:\n",
    "            down_out = down(down_out)\n",
    "            skip_connections.append(down_out)\n",
    "            down_out = self.pool(down_out)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(down_out)\n",
    "\n",
    "        # Decoder\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(len(self.ups)):\n",
    "            skip = skip_connections[idx]\n",
    "            x = self.ups[idx](x, skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def test_model(input_size=(64, 64)):\n",
    "    # Create sample input with smaller dimensions\n",
    "    batch_size = 2\n",
    "    channels = 3\n",
    "    height, width = input_size\n",
    "    \n",
    "    # Initialize model with reduced feature dimensions\n",
    "    model = MambaAttentionUNET(\n",
    "        in_channels=channels, \n",
    "        out_channels=1,\n",
    "        features=[32, 64, 128, 256]  # Reduced feature dimensions\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create input tensor\n",
    "    x = torch.randn(batch_size, channels, height, width).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.no_grad():\n",
    "        output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model, output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with smaller input size first\n",
    "    print(\"Testing model with small input...\")\n",
    "    model, output = test_model(input_size=(64, 64))\n",
    "    print(\"Small input test successful!\")\n",
    "    \n",
    "    # If successful, can try with larger input\n",
    "    try:\n",
    "        print(\"\\nTesting model with medium input...\")\n",
    "        model, output = test_model(input_size=(128, 128))\n",
    "        print(\"Medium input test successful!\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory error with medium input, stick with smaller input size\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Function to summarize the model\n",
    "def model_summary(model, input_size):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    summary(model, input_size)\n",
    "\n",
    "# Function to visualize the model\n",
    "def visualize_model(model, input_tensor):\n",
    "    model.eval()\n",
    "    output = model(input_tensor)\n",
    "    dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             896\n",
      "         LayerNorm-2             [-1, 4096, 32]              64\n",
      "            Conv1d-3             [-1, 32, 4096]             128\n",
      "              GELU-4             [-1, 32, 4096]               0\n",
      "            Conv1d-5             [-1, 32, 4096]           1,056\n",
      "SimplifiedMambaBlock-6           [-1, 32, 64, 64]               0\n",
      "         LayerNorm-7             [-1, 4096, 32]              64\n",
      "            Linear-8             [-1, 4096, 96]           3,168\n",
      "            Linear-9             [-1, 4096, 32]           1,056\n",
      "EfficientAttentionBlock-10           [-1, 32, 64, 64]               0\n",
      "      BatchNorm2d-11           [-1, 32, 64, 64]              64\n",
      "             GELU-12           [-1, 32, 64, 64]               0\n",
      "        DownBlock-13           [-1, 32, 64, 64]               0\n",
      "        MaxPool2d-14           [-1, 32, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          18,496\n",
      "        LayerNorm-16             [-1, 1024, 64]             128\n",
      "           Conv1d-17             [-1, 64, 1024]             256\n",
      "             GELU-18             [-1, 64, 1024]               0\n",
      "           Conv1d-19             [-1, 64, 1024]           4,160\n",
      "SimplifiedMambaBlock-20           [-1, 64, 32, 32]               0\n",
      "        LayerNorm-21             [-1, 1024, 64]             128\n",
      "           Linear-22             [-1, 1024, 96]           6,240\n",
      "           Linear-23             [-1, 1024, 64]           2,112\n",
      "EfficientAttentionBlock-24           [-1, 64, 32, 32]               0\n",
      "      BatchNorm2d-25           [-1, 64, 32, 32]             128\n",
      "             GELU-26           [-1, 64, 32, 32]               0\n",
      "        DownBlock-27           [-1, 64, 32, 32]               0\n",
      "        MaxPool2d-28           [-1, 64, 16, 16]               0\n",
      "           Conv2d-29          [-1, 128, 16, 16]          73,856\n",
      "        LayerNorm-30             [-1, 256, 128]             256\n",
      "           Conv1d-31             [-1, 128, 256]             512\n",
      "             GELU-32             [-1, 128, 256]               0\n",
      "           Conv1d-33             [-1, 128, 256]          16,512\n",
      "SimplifiedMambaBlock-34          [-1, 128, 16, 16]               0\n",
      "        LayerNorm-35             [-1, 256, 128]             256\n",
      "           Linear-36              [-1, 256, 96]          12,384\n",
      "           Linear-37             [-1, 256, 128]           4,224\n",
      "EfficientAttentionBlock-38          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-39          [-1, 128, 16, 16]             256\n",
      "             GELU-40          [-1, 128, 16, 16]               0\n",
      "        DownBlock-41          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-42            [-1, 128, 8, 8]               0\n",
      "           Conv2d-43            [-1, 256, 8, 8]         295,168\n",
      "        LayerNorm-44              [-1, 64, 256]             512\n",
      "           Conv1d-45              [-1, 256, 64]           1,024\n",
      "             GELU-46              [-1, 256, 64]               0\n",
      "           Conv1d-47              [-1, 256, 64]          65,792\n",
      "SimplifiedMambaBlock-48            [-1, 256, 8, 8]               0\n",
      "        LayerNorm-49              [-1, 64, 256]             512\n",
      "           Linear-50               [-1, 64, 96]          24,672\n",
      "           Linear-51              [-1, 64, 256]           8,448\n",
      "EfficientAttentionBlock-52            [-1, 256, 8, 8]               0\n",
      "      BatchNorm2d-53            [-1, 256, 8, 8]             512\n",
      "             GELU-54            [-1, 256, 8, 8]               0\n",
      "        DownBlock-55            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-56            [-1, 256, 4, 4]               0\n",
      "           Conv2d-57            [-1, 512, 4, 4]       1,180,160\n",
      "        LayerNorm-58              [-1, 16, 512]           1,024\n",
      "           Conv1d-59              [-1, 512, 16]           2,048\n",
      "             GELU-60              [-1, 512, 16]               0\n",
      "           Conv1d-61              [-1, 512, 16]         262,656\n",
      "SimplifiedMambaBlock-62            [-1, 512, 4, 4]               0\n",
      "        LayerNorm-63              [-1, 16, 512]           1,024\n",
      "           Linear-64              [-1, 16, 192]          98,496\n",
      "           Linear-65              [-1, 16, 512]          33,280\n",
      "EfficientAttentionBlock-66            [-1, 512, 4, 4]               0\n",
      "      BatchNorm2d-67            [-1, 512, 4, 4]           1,024\n",
      "             GELU-68            [-1, 512, 4, 4]               0\n",
      "        DownBlock-69            [-1, 512, 4, 4]               0\n",
      "  ConvTranspose2d-70            [-1, 256, 8, 8]         524,544\n",
      "           Conv2d-71            [-1, 256, 8, 8]       1,179,904\n",
      "        LayerNorm-72              [-1, 64, 256]             512\n",
      "           Conv1d-73              [-1, 256, 64]           1,024\n",
      "             GELU-74              [-1, 256, 64]               0\n",
      "           Conv1d-75              [-1, 256, 64]          65,792\n",
      "SimplifiedMambaBlock-76            [-1, 256, 8, 8]               0\n",
      "        LayerNorm-77              [-1, 64, 256]             512\n",
      "           Linear-78               [-1, 64, 96]          24,672\n",
      "           Linear-79              [-1, 64, 256]           8,448\n",
      "EfficientAttentionBlock-80            [-1, 256, 8, 8]               0\n",
      "      BatchNorm2d-81            [-1, 256, 8, 8]             512\n",
      "             GELU-82            [-1, 256, 8, 8]               0\n",
      "          UpBlock-83            [-1, 256, 8, 8]               0\n",
      "  ConvTranspose2d-84          [-1, 128, 16, 16]         131,200\n",
      "           Conv2d-85          [-1, 128, 16, 16]         295,040\n",
      "        LayerNorm-86             [-1, 256, 128]             256\n",
      "           Conv1d-87             [-1, 128, 256]             512\n",
      "             GELU-88             [-1, 128, 256]               0\n",
      "           Conv1d-89             [-1, 128, 256]          16,512\n",
      "SimplifiedMambaBlock-90          [-1, 128, 16, 16]               0\n",
      "        LayerNorm-91             [-1, 256, 128]             256\n",
      "           Linear-92              [-1, 256, 96]          12,384\n",
      "           Linear-93             [-1, 256, 128]           4,224\n",
      "EfficientAttentionBlock-94          [-1, 128, 16, 16]               0\n",
      "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
      "             GELU-96          [-1, 128, 16, 16]               0\n",
      "          UpBlock-97          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-98           [-1, 64, 32, 32]          32,832\n",
      "           Conv2d-99           [-1, 64, 32, 32]          73,792\n",
      "       LayerNorm-100             [-1, 1024, 64]             128\n",
      "          Conv1d-101             [-1, 64, 1024]             256\n",
      "            GELU-102             [-1, 64, 1024]               0\n",
      "          Conv1d-103             [-1, 64, 1024]           4,160\n",
      "SimplifiedMambaBlock-104           [-1, 64, 32, 32]               0\n",
      "       LayerNorm-105             [-1, 1024, 64]             128\n",
      "          Linear-106             [-1, 1024, 96]           6,240\n",
      "          Linear-107             [-1, 1024, 64]           2,112\n",
      "EfficientAttentionBlock-108           [-1, 64, 32, 32]               0\n",
      "     BatchNorm2d-109           [-1, 64, 32, 32]             128\n",
      "            GELU-110           [-1, 64, 32, 32]               0\n",
      "         UpBlock-111           [-1, 64, 32, 32]               0\n",
      " ConvTranspose2d-112           [-1, 32, 64, 64]           8,224\n",
      "          Conv2d-113           [-1, 32, 64, 64]          18,464\n",
      "       LayerNorm-114             [-1, 4096, 32]              64\n",
      "          Conv1d-115             [-1, 32, 4096]             128\n",
      "            GELU-116             [-1, 32, 4096]               0\n",
      "          Conv1d-117             [-1, 32, 4096]           1,056\n",
      "SimplifiedMambaBlock-118           [-1, 32, 64, 64]               0\n",
      "       LayerNorm-119             [-1, 4096, 32]              64\n",
      "          Linear-120             [-1, 4096, 96]           3,168\n",
      "          Linear-121             [-1, 4096, 32]           1,056\n",
      "EfficientAttentionBlock-122           [-1, 32, 64, 64]               0\n",
      "     BatchNorm2d-123           [-1, 32, 64, 64]              64\n",
      "            GELU-124           [-1, 32, 64, 64]               0\n",
      "         UpBlock-125           [-1, 32, 64, 64]               0\n",
      "          Conv2d-126            [-1, 1, 64, 64]              33\n",
      "================================================================\n",
      "Total params: 4,541,409\n",
      "Trainable params: 4,541,409\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 56.12\n",
      "Params size (MB): 17.32\n",
      "Estimated Total Size (MB): 73.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define input size\n",
    "input_size = (3, 64, 64)  # Example: (channels, height, width)\n",
    "\n",
    "# Create the model\n",
    "model = MambaAttentionUNET(\n",
    "    in_channels=input_size[0], \n",
    "    out_channels=1, \n",
    "    features=[32, 64, 128, 256]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "print(\"Model Summary:\")\n",
    "model_summary(model, input_size)\n",
    "\n",
    "# Create a dummy input tensor for visualization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_tensor = torch.randn(1, *input_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating model visualization...\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\backend\\execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save the visualization as a file (e.g., PNG)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dot\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmamba_attention_unet_graph\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel visualization saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmamba_attention_unet_graph.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\backend\\rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\graphviz\\backend\\execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the visualization graph\n",
    "print(\"\\nGenerating model visualization...\")\n",
    "dot = visualize_model(model, input_tensor)\n",
    "\n",
    "# Save the visualization as a file (e.g., PNG)\n",
    "dot.format = \"png\"\n",
    "dot.render(\"mamba_attention_unet_graph\")\n",
    "print(\"Model visualization saved as 'mamba_attention_unet_graph.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
